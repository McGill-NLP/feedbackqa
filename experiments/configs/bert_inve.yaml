train_file:
  train_file_1: '/home/lcc/rqa_feedback/data/crowdsourced_data/Australia/train.json'
  train_file_2: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/train.json'
  train_file_3: '/home/lcc/rqa_feedback/data/crowdsourced_data/UK/train.json'
  train_file_4: '/home/lcc/rqa_feedback/data/crowdsourced_data/WHO/train.json'
  train_file_5: '/home/lcc/rqa_feedback/data/crowdsourced_data/Quebec/train.json'
# train_file: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/train.json'
dev_files:
  dev_file_1: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/valid.json'
  dev_file_2: '/home/lcc/rqa_feedback/data/crowdsourced_data/Australia/valid.json'
  dev_file_3: '/home/lcc/rqa_feedback/data/crowdsourced_data/UK/valid.json'
  dev_file_4: '/home/lcc/rqa_feedback/data/crowdsourced_data/WHO/valid.json'
  dev_file_5: '/home/lcc/rqa_feedback/data/crowdsourced_data/Quebec/valid.json'
test_file: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/test.json'
sampling_ratios:
  - 33
  - 36
  - 42
  - 21
  - 73

cache_folder: 'cached_natq'
tokenizer_name: 'bert-base-uncased'
keep_ood: false

batch_size: 16
valid_batch_size: 64
accumulate_grad_batches: 8
gradient_clipping: 0
distributed_backend: ddp

loss_type: classification
max_epochs: 40
max_paragraph_len: 512
max_question_len: 50
train_source: batch
eval_source: fixed

logging:
  logger: 'wandb'
  project: 'inverse_polyencoder'
  entity: 'covid_chatbot'
  group: 'multidomain'
  name: 'QA_bert_base'

model:
  bert_base: bert-base-uncased
  dropout: 0.1
  dropout_bert: null
  freeze_bert: false
  layers_post_pooling: []
  layers_pre_pooling: []
  name: inverse_polyencoder
  polyencoder_type: codes
  poly_n_codes: 64
  poly_attention_type: basic
  poly_attention_num_heads: 4
  codes_attention_type: basic
  codes_attention_num_heads: 4
  embedding_size: 768
  normalize_model_result: false
  pooling_type: avg
  single_encoder: true

optimizer:
  lr: 5.0e-05
  name: adamw
patience: 15
precision: 16
seed: null

use_parlai: False
parlai:
  model: transformer/polyencoder
  model_file: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model'
  init_model: zoo:pretrained_transformers/poly_model_huge_wikito/model
  dict_class: 'parlai.core.dict:DictionaryAgent'
  dict_file: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model.dict'
  load_from_checkpoint: False
  dict_initpath: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model.dict'
  dict_language: english
  dict_max_ngram_size: -1
  dict_minfreq: 0
  dict_maxtokens: -1
  dict_nulltoken: '__null__'
  dict_starttoken: '__start__'
  dict_endtoken: '__start__'
  dict_unktoken: '__unk__'
  dict_tokenizer: 'bpe'
  dict_textfields: 'text,labels'
  dict_lower: True
  dict_maxexs: -1

  variant: xlm
  reduction_type: mean
  share_encoders: False
  learn_positional_embeddings: True
  n_layers: 12
  n_heads: 12
  ffn_size: 3072
  attention_dropout: 0.1
  relu_dropout: 0.0
  dropout: 0.1
  n_positions: 1024
  embedding_size: 768
  activation: gelu
  embeddings_scale: False
  n_segments: 2
  learn_embeddings: True
  polyencoder_type: codes
  poly_n_codes: 64
  poly_attention_type: basic
  dict_endtoken: '__start__'
  fp16: True
