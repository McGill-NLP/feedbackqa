train_file:
  train_file_1: '/home/lcc/rqa_feedback/data/crowdsourced_data/Australia/feedback_train.json'
  train_file_2: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/feedback_train.json'
  train_file_3: '/home/lcc/rqa_feedback/data/crowdsourced_data/UK/feedback_train.json'
  train_file_4: '/home/lcc/rqa_feedback/data/crowdsourced_data/WHO/feedback_train.json' 
# train_file: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/feedback_train.json'
dev_files:
  dev_file_1: '/home/lcc/rqa_feedback/data/crowdsourced_data/Australia/feedback_valid.json'
  dev_file_2: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/feedback_valid.json'
  dev_file_3: '/home/lcc/rqa_feedback/data/crowdsourced_data/UK/feedback_valid.json'
  dev_file_4: '/home/lcc/rqa_feedback/data/crowdsourced_data/WHO/feedback_valid.json'
test_file: '/home/lcc/rqa_feedback/data/crowdsourced_data/CDC/test.json'
sampling_ratios:
  - 36
  - 40
  - 23
  - 47

cache_folder: 'cached_natq'
tokenizer_name: 'facebook/bart-large'
keep_ood: false

batch_size: 8
valid_batch_size: 5
accumulate_grad_batches: 3
gradient_clipping: 0
distributed_backend: ddp

loss_type: classification
max_epochs: 6
max_paragraph_len: 512
max_question_len: 32
train_source: batch
eval_source: fixed

logging:
  logger: 'wandb'
  project: 'bart_rate_kl'
  entity: 'covid_chatbot'
  group: 'multidomain'
  name: 'Rating_bart_base_kl'

model:
  bert_base: facebook/bart-large
  dropout: 0.149
  dropout_bert: null
  freeze_bert: false
  layers_post_pooling: []
  layers_pre_pooling: []
  name: inverse_polyencoder
  polyencoder_type: codes
  poly_n_codes: 64
  poly_attention_type: basic
  poly_attention_num_heads: 4
  codes_attention_type: basic
  codes_attention_num_heads: 4
  embedding_size: 786
  normalize_model_result: false
  pooling_type: avg
  single_encoder: true
  encoder_only: false
  single_decoder: true

optimizer:
  lr: 0.00003816
  name: adamw
patience: 15
precision: 32
seed: null

use_parlai: False
parlai:
  model: transformer/polyencoder
  model_file: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model'
  init_model: zoo:pretrained_transformers/poly_model_huge_wikito/model
  dict_class: 'parlai.core.dict:DictionaryAgent'
  dict_file: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model.dict'
  load_from_checkpoint: False
  dict_initpath: '/home/prakhar/ParlAI/data/models/pretrained_transformers/poly_model_huge_wikito/model.dict'
  dict_language: english
  dict_max_ngram_size: -1
  dict_minfreq: 0
  dict_maxtokens: -1
  dict_nulltoken: '__null__'
  dict_starttoken: '__start__'
  dict_endtoken: '__start__'
  dict_unktoken: '__unk__'
  dict_tokenizer: 'bpe'
  dict_textfields: 'text,labels'
  dict_lower: True
  dict_maxexs: -1

  variant: xlm
  reduction_type: mean
  share_encoders: False
  learn_positional_embeddings: True
  n_layers: 12
  n_heads: 12
  ffn_size: 3072
  attention_dropout: 0.1
  relu_dropout: 0.0
  dropout: 0.1
  n_positions: 1024
  embedding_size: 1024
  activation: gelu
  embeddings_scale: False
  n_segments: 2
  learn_embeddings: True
  polyencoder_type: codes
  poly_n_codes: 64
  poly_attention_type: basic
  dict_endtoken: '__start__'
  fp16: True
